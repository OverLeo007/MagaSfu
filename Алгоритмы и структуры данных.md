# Билет 1

## Абстрактные типы данных в программировании. Ассоциативные массивы и списки.

**Определить их словесно или с использованием схем, основные операции над ними. Показать их вычислительную сложность.**

---

## Абстрактные типы данных (АТД)
Абстрактный тип данных (АТД) — это логическая модель структуры данных, которая определяет допустимые операции и их поведение, **не раскрывая конкретную реализацию**.
АТД задаёт интерфейс, а не реализацию.  
Примеры: список, стек, очередь, множество, словарь.

---
## Список (List)
Список — это упорядоченная коллекция элементов, в которой:
- Каждый элемент имеет индекс
- Допускаются повторяющиеся значения

### Основные операции и их сложности (при реализации через массив):

|Операция|Описание|Сложность|
|---|---|---|
|`A[i]`|доступ по индексу|`O(1)`|
|`append(x)`|добавление в конец|`O(1)` (в среднем)|
|`insert(i, x)`|вставка в начало/середину|`O(n)`|
|`pop(i)` / `del A[i]`|удаление по индексу|`O(n)`|
|`x in A`|поиск по значению|`O(n)`|

Типичные реализации: динамический массив, односвязный или двусвязный список.

---
## Ассоциативный массив (словарь / dictionary / map)

Ассоциативный массив — это структура данных, хранящая пары **ключ → значение**.  
Ключи уникальны, доступ к значению осуществляется по ключу.

### Основные операции и их сложности (в реализации через хеш-таблицу):
|Операция|Пример|Сложность|
|---|---|---|
|доступ по ключу|`D[k]`|`O(1)` (в среднем)|
|добавление|`D[k] = v`|`O(1)`|
|удаление|`del D[k]`|`O(1)`|
|проверка наличия ключа|`k in D`|`O(1)`|
|перебор всех элементов|`for k, v in D.items()`|`O(n)`|

Типичные реализации: хеш-таблица (средняя сложность `O(1)`), реже — сбалансированное дерево (`O(log n)`).

---

## Сравнение списка и словаря

|Свойство|Список|Ассоциативный массив|
|---|---|---|
|Доступ|По индексу|По ключу|
|Упорядоченность|Есть|Не гарантируется (в хеш-таблице)|
|Повторы|Разрешены|Ключи уникальны|
|Быстрый доступ|`O(1)` по индексу|`O(1)` по ключу|
|Перебор|По порядку|В произвольном порядке|

---

## Выводы

- АТД описывают, **что можно делать с данными**, а не как именно они устроены.
- Список — простая структура с доступом по индексу, удобен для последовательных данных.
- Ассоциативный массив — структура с доступом по ключу, эффективна для поиска и отображений.
- Оба типа данных — фундаментальны и часто используются в программировании.
# Билет 2

## Древовидные структуры (деревья бинарные, сбалансированные, сильноветвящиеся).

**Определить их словесно или с использованием схем. Основные операции над ними (поиск, вставка, удаление). Показать вычислительную сложность.**

---

## Дерево

Дерево — это иерархическая структура данных, в которой:
- Каждый элемент называется **узлом**
- Один узел — **корень**
- У каждого узла могут быть **потомки** (дети)
- Связи между узлами называются **рёбрами**
---
## Бинарное дерево
Бинарное дерево — дерево, где каждый узел имеет не более двух потомков: **левый** и **правый**.
**Бинарное дерево поиска (BST)** — структура, в которой:
- Все значения в левом поддереве меньше текущего узла
- Все значения в правом поддереве больше текущего узла
### Сложности (в среднем):
- Поиск: `O(log n)`
- Вставка: `O(log n)`
- Удаление: `O(log n)`
### В худшем случае (несбалансированное дерево):
- Все операции — `O(n)`
---
## Сбалансированное дерево
Сбалансированное дерево — это бинарное дерево, у которого высота левого и правого поддеревьев отличается не более чем на 1 (например, AVL-дерево или красно-чёрное дерево).
**Преимущество:** гарантированная логарифмическая высота.
### Сложности (всегда):
- Поиск: `O(log n)`
- Вставка: `O(log n)`
- Удаление: `O(log n)
Баланс поддерживается с помощью поворотов при вставке и удалении
---
## Сильноветвящиеся деревья
Сильноветвящееся (n-арное) дерево — это дерево, в котором каждый узел может иметь более двух детей.
Пример: **B-деревья** — используются в базах данных и файловых системах.
### Сложности (в среднем и в худшем случае при балансировке):
- Поиск: `O(log n)`
- Вставка: `O(log n)`
- Удаление: `O(log n)`
У таких деревьев меньше высота, за счёт широкой ветвистости.
---
## Сравнение деревьев

|Вид дерева|Потомков|Балансировка|Высота|Сложность операций|
|---|---|---|---|---|
|Бинарное|≤ 2|Нет|до `n`|До `O(n)`|
|Сбалансированное|≤ 2|Есть|`O(log n)`|`O(log n)`|
|Сильноветвящееся|> 2|Есть|`O(log n)`|`O(log n)`|

---
## Выводы
- Деревья позволяют эффективно хранить и искать данные в иерархической форме.
- Бинарные деревья просты, но могут деградировать без балансировки.
- Сбалансированные деревья обеспечивают надёжную производительность.
- Сильноветвящиеся деревья лучше подходят для работы с большими объёмами данных.

# Билет 3

## Понятие рекурсии в программировании. Рекурсивные и нерекурсивные алгоритмы.

**Условия окончания работы. Привести примеры рекурсивного и нерекурсивного алгоритмов, соответственно (словесно или с использованием схем). Показать их вычислительную сложность.**

---

## Рекурсия

Рекурсия — это приём в программировании, при котором **функция вызывает саму себя** для решения подзадачи. Каждое новое рекурсивное вызов создаёт копию функции со своими локальными переменными.

---

## Условия корректной рекурсии
1. **Базовый случай** — условие, при котором рекурсивные вызовы прекращаются.
2. **Рекурсивный шаг** — переход к подзадаче меньшего размера.

Без базового случая произойдёт бесконечный цикл вызовов и переполнение стека.

---

## Пример рекурсивного алгоритма — факториал
Факториал числа `n` (`n!`) — это произведение всех чисел от `1` до `n`.
Рекурсивное определение:
- `0! = 1` (базовый случай)
- `n! = n * (n - 1)!` (рекурсивный шаг)

Рекурсивная реализация:
```python
def factorial(n):
    if n == 0:
        return 1
    return n * factorial(n - 1)
```
**Сложность по времени:** `O(n)`  
**Сложность по памяти:** `O(n)` (из-за стека вызовов)

---

## Пример нерекурсивного алгоритма — факториал (итеративно)

```python
def factorial_iter(n):
    result = 1
    for i in range(2, n + 1):
        result *= i
    return result
```
**Сложность по времени:** `O(n)`  
**Сложность по памяти:** `O(1)`

---
## Рекурсивные и нерекурсивные алгоритмы: сравнение

|Характеристика|Рекурсивный подход|Нерекурсивный (итеративный) подход|
|---|---|---|
|Простота записи|Чаще проще и короче|Иногда сложнее|
|Память|Использует стек вызовов|Постоянная (`O(1)`)|
|Скорость|Может быть медленнее|Часто быстрее|
|Риск переполнения стека|Есть|Нет|

---

## Выводы
- Рекурсия — мощный инструмент, особенно в задачах с естественной иерархией (деревья, графы).
- Главное — правильно определить базовый случай, иначе программа «зависнет».
- Почти любой рекурсивный алгоритм можно переписать в итеративной форме, иногда с явным использованием стека.
# Билет 4

## Формальная постановка задачи сортировки списков/массивов. Классификация алгоритмов сортировки по различным критериям. Прямые и улучшенные алгоритмы сортировки. Привести примеры прямого и улучшенного алгоритма сортировки, соответственно (словесно или с использованием схем). Показать их вычислительную сложность.

---

## Формальная постановка задачи сортировки

Дано: массив (или список) чисел `A = [a₁, a₂, ..., aₙ]`.  
Требуется: переставить элементы массива так, чтобы они шли в неубывающем порядке:

```
a₁ ≤ a₂ ≤ ... ≤ aₙ
```

---

## Классификация алгоритмов сортировки

**1. По способу реализации:**

- **Внутренние** — вся работа происходит в оперативной памяти.
    
- **Внешние** — применяются для сортировки данных, не помещающихся в память.
    

**2. По устойчивости:**

- **Устойчивые** — не меняют порядок равных элементов.
    
- **Неустойчивые** — могут менять порядок равных элементов.
    

**3. По сложности:**
- **Простые (прямые)** — `O(n²)` в среднем.
- **Улучшенные (эффективные)** — `O(n log n)` в среднем.

**4. По необходимости доп. памяти:**
- **Сортировки на месте** (in-place): не требуют значительной дополнительной памяти.
- **Не in-place**: используют вспомогательные структуры.

---

## Прямой алгоритм сортировки: сортировка выбором

**Идея:** находим наименьший элемент в оставшейся части массива и ставим его на нужное место.
**Пример:**
```
[5, 3, 1, 4]
→ [1, 3, 5, 4]
→ [1, 3, 5, 4]
→ [1, 3, 4, 5]
```
**Сложность:**
- Время: `O(n²)` — два вложенных цикла.
- Память: `O(1)` — in-place.

---

## Улучшенный алгоритм: быстрая сортировка (quicksort)
**Идея:** выбираем опорный элемент (pivot), разделяем массив на два подмассива: меньше и больше опорного, и рекурсивно сортируем их.
**Пример (словесно):**
```
Массив: [7, 2, 1, 9]
→ опорный элемент: 7
→ меньшие: [2, 1], большие: [9]
→ рекурсивно сортируем: [1, 2] + [7] + [9]
→ результат: [1, 2, 7, 9]
```
**Сложность:**
- Средняя: `O(n log n)`
- Худшая (при плохом выборе pivot): `O(n²)`
- Память: `O(log n)` (глубина рекурсии)

---

## Выводы
- Прямые сортировки — просты в реализации, но неэффективны при больших объемах данных.
- Улучшенные сортировки (quicksort, mergesort, heapsort) обеспечивают высокую производительность.
- На практике часто используют встроенные сортировки в языках программирования, основанные на гибридных подходах (например, Timsort в Python)
# Билет 5

## Формальная постановка задачи поиска образа в строке

**Классификация алгоритмов поиска образа. Прямые и улучшенные алгоритмы поиска образа. Привести примеры прямого и улучшенного алгоритма поиска образа в строке, соответственно (словесно или с использованием схем). Показать их вычислительную сложность.**

---

## Формальная постановка задачи
**Дано:**
- Строка (текст) `T` длины `n`
- Образец (подстрока) `P` длины `m`
**Найти:**  
Все индексы `i`, такие что `T[i..i+m−1] = P[0..m−1]`, где `0 ≤ i ≤ n−m`

---

## Классификация алгоритмов поиска образа
1. **Прямые (наивные):**
    - Последовательное сравнение образца с каждой возможной подстрокой текста
    - Не используют предварительной информации
2. **Улучшенные:**
    - Используют предварительный анализ образца или текста
    - Сокращают количество сравнений или сдвигов
    - Примеры: Кнута–Морриса–Пратта, Бойера–Мура, Рабина–Карпа

---

## Прямой алгоритм: наивный поиск
**Идея:**  
Сравниваем образец `P` с каждой подстрокой `T[i..i+m−1]`, сдвигая образец на 1 позицию вправо.
**Алгоритм:**
```python
for i in range(n - m + 1):
    if T[i:i + m] == P:
        print(i)
```
**Сложность:**
- В худшем случае: `O(n·m)`
- В лучшем случае (вхождение в начале): `O(m)`
**Пример:**
```
T:  A B A C A B A B A B C A
P:          A B A B
```

---

## Улучшенный алгоритм: Кнута–Морриса–Пратта (КМП)
**Идея:**  
Избегать повторных сравнений за счёт предварительной обработки образца. Строится префикс-функция, показывающая, куда можно сдвинуть образец после несовпадения.
**Префикс-функция `π[i]`:**  
Длина наибольшего собственного суффикса `P[0..i]`, совпадающего с префиксом.
**Построение π:**
```python
π[0] = 0
for i in 1..m-1:
    j = π[i-1]
    while j > 0 and P[i] != P[j]:
        j = π[j-1]
    if P[i] == P[j]:
        j += 1
    π[i] = j
```
**Поиск:**
```python
i = 0, j = 0
while i < n:
    if T[i] == P[j]:
        i += 1
        j += 1
        if j == m:
            print(i - m)
            j = π[j - 1]
    elif j > 0:
        j = π[j - 1]
    else:
        i += 1
```
**Сложность:**
- Предобработка: `O(m)`
- Поиск: `O(n)`
- Общая: `O(n + m)`
**Пример:**
```
T:       A B A C A B A B A B C
P:         A B A B
π:         0 0 1 2
```

---

## Другие улучшенные алгоритмы (упоминание)
- **Бойера–Мура:**  
    Использует эвристики плохого символа и хорошего суффикса. Очень эффективен на практике. Сложность в худшем случае: `O(n·m)`, в среднем: `O(n)`
- **Рабина–Карпа:**  
    Применяет хеширование подстрок. Подходит для множественного поиска. Средняя сложность: `O(n + m)`, в худшем случае: `O(n·m)`

---
## Выводы
- Поиск образа в строке — классическая задача обработки текста.
- Прямые алгоритмы просты, но неэффективны при больших объемах.
- Улучшенные алгоритмы уменьшают число сравнений с помощью предобработки.
- КМП — универсальный и линейный по времени алгоритм.
- Выбор метода зависит от задачи: простота реализации против скорости.
# Билет 6

## Понятие и формальное определение графа. Представление графов в памяти вычислительной машины. Обход графа в глубину, обход графа в ширину. Примеры алгоритмов обхода и их сложность.

---

## Понятие и формальное определение графа
**Граф** — это математическая структура, состоящая из множества **вершин** (узлов) и множества **рёбер** (связей между вершинами).
**Формально:**  
Граф `G = (V, E)`, где:
- `V` — множество вершин, `|V| = n`
- `E` — множество рёбер, каждое ребро — пара `(u, v)`, где `u, v ∈ V`
**Типы графов:**
- **Ориентированный граф:** ребра имеют направление `(u → v)`
- **Неориентированный граф:** ребра не имеют направления
- **Взвешенный граф:** рёбра имеют веса (стоимости)

---

## Представление графов в памяти вычислительной машины
1. **Матрица смежности:**
    - Квадратная матрица `n×n`, где элемент `A[i][j] = 1`, если есть ребро из вершины `i` в `j`, иначе `0`
    - Удобно для плотных графов
    - Затраты памяти: `O(n²)`
2. **Список смежности:**
    - Для каждой вершины хранится список соседних вершин
    - Удобно для разреженных графов
    - Затраты памяти: `O(n + |E|)`

---

## Обход графа в глубину (DFS)

**Идея:**  
Рекурсивно посещаем вершины, погружаясь "вглубь" графа, пока не достигнем вершины без непосещённых соседей, затем откатываемся назад.

**Алгоритм (рекурсивный):**

```
DFS(v):
    пометить v как посещённую
    для каждого соседа u вершины v:
        если u не посещена:
            DFS(u)
```

**Пример схемы:**

```
   A
  / \
 B   C
     |
     D
```

Порядок обхода: A → B → C → D

**Сложность:**

- Время: `O(n + |E|)
- Память: `O(n)` (стек вызовов и массив посещений)
    

---

## Обход графа в ширину (BFS)

**Идея:**  
Посещаем вершины слоями: сначала стартовую вершину, затем все её соседи, потом соседей соседей и т.д.

**Алгоритм:**

```
BFS(s):
    создать очередь Q
    пометить s как посещённую и добавить в Q
    пока Q не пуст:
        v = Q.dequeue()
        для каждого соседа u вершины v:
            если u не посещена:
                пометить u и добавить в Q
```

**Пример схемы:**

```
   A
  / \
 B   C
     |
     D
```

Порядок обхода: A → B → C → D

**Сложность:**

- Время: `O(n + |E|)`
    
- Память: `O(n)` (очередь и массив посещений)
    

---

## Выводы

- Граф — множество вершин и рёбер с направлениями или без.
    
- Для хранения используются матрица смежности (память `O(n²)`) и списки смежности (`O(n + |E|)`).
    
- DFS — рекурсивный обход "вглубь", эффективен для поиска путей и компонентов связности.
    
- BFS — пошаговый обход "вширь", используется для поиска кратчайших путей в невзвешенных графах.
    
- Сложности обоих алгоритмов: `O(n + |E|)`.
# Билет 7

## Формальная постановка задачи поиска кратчайших путей в графе. Описать алгоритмы Дейкстры и Флойда. Показать их вычислительную сложность.

---

## Формальная постановка задачи

Дано:

- Взвешенный граф `G = (V, E)`, где каждому ребру `(u, v)` соответствует вес (стоимость) `w(u, v)`
    
- Источник (начальная вершина) `s ∈ V`
    

Задача:

- Найти кратчайшие пути от `s` до всех остальных вершин графа
    
- Для алгоритма Флойда — найти кратчайшие пути между всеми парами вершин
    

---

## Алгоритм Дейкстры

**Назначение:** поиск кратчайших путей от одной вершины `s` до всех остальных в графе с неотрицательными весами.

**Идея:**

- Инициализировать расстояния: `dist[s] = 0`, для остальных — `∞`
    
- На каждом шаге выбирать вершину с минимальным текущим расстоянием, обновлять расстояния до её соседей
    
- Использует структуру данных — очередь с приоритетом (min-heap)
    

**Псевдокод:**

```
dist[s] = 0
Q = множество всех вершин

while Q не пусто:
    u = вершина с минимальным dist[u] в Q
    удалить u из Q
    для каждого соседа v вершины u:
        если dist[u] + w(u,v) < dist[v]:
            dist[v] = dist[u] + w(u,v)
```

**Сложность:**

- При использовании массива: `O(n²)`
    
- При использовании кучи (min-heap): `O((n + |E|) log n)`
    

---

## Алгоритм Флойда (Флойд-Уоршелл)

**Назначение:** поиск кратчайших путей между всеми парами вершин в графе (взвешенном, с возможными отрицательными весами, но без отрицательных циклов).

**Идея:**

- Использует динамическое программирование
    
- Постепенно учитывает промежуточные вершины, улучшая оценки расстояний между парами
    

**Псевдокод:**

```
Инициализация:  
dist[i][j] = вес ребра (i, j), если ребро есть, иначе ∞  
dist[i][i] = 0 для всех i

for k in 1..n:
    for i in 1..n:
        for j in 1..n:
            dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])
```

**Сложность:**

- Время: `O(n³)`
    
- Память: `O(n²)` (матрица расстояний)
    

---

## Выводы

- Задача поиска кратчайших путей — ключевая для анализа графов.
    
- Алгоритм Дейкстры эффективен для одного источника и неотрицательных весов, работает за `O((n + |E|) log n)` с кучей.
    
- Алгоритм Флойда подходит для всех пар вершин, включая отрицательные веса (без отрицательных циклов), но медленнее — `O(n³)`.
    
- Выбор алгоритма зависит от задачи: один источник и большие графы — Дейкстра; все пары и небольшие графы — Флойд.


# Билет 8

## Формальная постановка задачи построения минимального остовного дерева графа. Описать алгоритмы Прима и Крускала. Показать их вычислительную сложность.

---

## Формальная постановка задачи

Дано:

- Взвешенный неориентированный связный граф `G = (V, E)`, где `V` — вершины, `E` — рёбра с весами `w(e)`
    

Задача:

- Построить **минимальное остовное дерево (МОТ)** — связный ацикличный подграф, содержащий все вершины `V`, с минимальной суммой весов рёбер
    

---

## Алгоритм Прима

**Идея:**

- Начинаем с произвольной вершины
    
- Итеративно добавляем к остову минимальное ребро, связывающее уже включённые вершины с остальными
    
- Используется приоритетная очередь для выбора минимального ребра
    

**Псевдокод:**

```
Выбрать стартовую вершину s
Множество MST = {s}
Множество ребер E' = рёбра, исходящие из s

while MST содержит не все вершины:
    выбрать ребро (u, v) минимального веса из E', где u ∈ MST, v ∉ MST
    добавить v в MST
    добавить в E' все рёбра, исходящие из v, ведущие во вне MST
```

**Сложность:**

- С использованием кучи и списка смежности: `O(|E| log |V|)`
    

---

## Алгоритм Крускала

**Идея:**

- Сортируем все рёбра по весу
    
- Итеративно добавляем ребро с минимальным весом, если оно не образует цикл (проверка с помощью структуры **Disjoint Set Union (DSU)**)
    
- Построение остова через объединение компонент
    

**Псевдокод:**

```
Отсортировать рёбра по возрастанию веса
Инициализировать DSU для вершин
MST = пустое множество

for ребро (u, v) в порядке возрастания веса:
    если u и v в разных компонентах:
        добавить ребро в MST
        объединить компоненты u и v
```

**Сложность:**

- Сортировка рёбер: `O(|E| log |E|)`
    
- Операции DSU: почти `O(|E|)` (амортизированно)
    
- Общая: `O(|E| log |E|)`
    

---

## Выводы

- Минимальное остовное дерево — связный подграф с минимальной суммой весов и без циклов.
    
- Алгоритм Прима растёт от одной вершины, выбирая минимальные ребра на границе.
    
- Алгоритм Крускала строит МОТ, выбирая минимальные ребра по весу, избегая циклов с помощью DSU.
    
- Сложности обоих алгоритмов примерно `O(|E| log |V|)` — эффективны для разреженных графов.
    
- Выбор алгоритма зависит от структуры графа и удобства реализации.

# Билет 9

## Методы сжатия данных без потерь. Привести пример алгоритма сжатия без потерь. Определить его вычислительную сложность.

---

## Методы сжатия данных без потерь

**Определение:**  
Сжатие без потерь — метод уменьшения объёма данных, при котором исходная информация восстанавливается полностью без ошибок.

**Основные подходы:**

1. **Статистическое кодирование** — использует частоты символов
    
2. **Словарное кодирование** — заменяет повторяющиеся фрагменты ссылками
    
3. **Кодирование на основе преобразований** — улучшает сжимаемость за счёт перестановок данных
    

---

## Пример алгоритма сжатия без потерь: алгоритм Хаффмана

**Идея:**

- Строится двоичное дерево, где символы с высокой частотой кодируются короткими битами, а редкие — длинными
    
- Кодирование переменной длины оптимально с точки зрения среднего количества бит на символ
    

**Основные шаги:**

1. Определить частоту каждого символа во входных данных
    
2. Создать очередь с приоритетом, помещая туда узлы — символы с их частотами
    
3. Итеративно извлекать два узла с минимальными частотами и объединять их в новый узел с суммарной частотой
    
4. Дерево строится до тех пор, пока не останется один корень — дерево Хаффмана
    
5. Каждый символ кодируется по пути от корня до листа: влево — 0, вправо — 1
    

**Пример схемы:**

```
Частоты: A:5, B:9, C:12, D:13, E:16, F:45

Построение дерева:
1) Объединяем A(5) и B(9) → узел 14
2) Объединяем C(12) и D(13) → узел 25
3) Объединяем узлы 14 и E(16) → узел 30
4) Объединяем узлы 25 и 30 → узел 55
5) Объединяем узел 55 и F(45) → корень 100
```

---

## Вычислительная сложность алгоритма Хаффмана

- Подсчёт частот: `O(n)`, где `n` — длина входных данных
    
- Построение дерева с использованием очереди с приоритетом:
    
    - `O(k log k)`, где `k` — количество уникальных символов
        
- Кодирование: `O(n)`
    

**Общая:**  
`O(n + k log k)` — эффективно для большинства применений

---

## Выводы

- Сжатие без потерь восстанавливает исходные данные полностью.
    
- Алгоритм Хаффмана — классический пример, оптимально кодирует символы переменной длины.
    
- Работает быстро, особенно при небольшом алфавите.
    
- Методы без потерь важны для текстов, программ и других данных, где важна точность.
# Билет 10

## Методы сжатия данных с потерями. Привести пример алгоритма сжатия с потерями. Показать его вычислительную сложность.

---

## Методы сжатия данных с потерями

**Определение:**  
Сжатие с потерями — метод уменьшения объёма данных, при котором часть исходной информации теряется, и восстановленные данные не идентичны исходным, но приемлемы по качеству.

**Основные направления:**

- Удаление избыточной или мало заметной информации
    
- Квантование, аппроксимация, преобразования
    

**Применение:**

- Изображения (JPEG), аудио (MP3), видео (MPEG)
    

---

## Пример алгоритма сжатия с потерями: JPEG

**Идея:**

- Разбить изображение на блоки (обычно 8×8 пикселей)
    
- Выполнить преобразование Дискретного Косинусного Преобразования (ДКП) каждого блока
    
- Квантовать коэффициенты ДКП, уменьшая точность менее заметных деталей
    
- Применить кодирование без потерь (например, Хаффман) для сжатия квантованных данных
    

**Основные шаги:**

1. **Разбиение:** изображение → блоки 8×8
    
2. **ДКП:** преобразование каждого блока из пикселей в частотные коэффициенты
    
3. **Квантование:** деление коэффициентов на заданные значения и округление — уменьшение точности
    
4. **Кодирование:** сжатие квантованных коэффициентов без потерь
    

**Схема:**

```
Изображение → Блоки → ДКП → Квантование → Кодирование → Сжатый файл
```

---

## Вычислительная сложность JPEG

- ДКП для одного блока 8×8: `O(64 log 64)` ≈ `O(64)` (фиксированная константа)
    
- Для всего изображения размером `N×N` пикселей: `O((N/8)^2 * 64) = O(N²)`
    
- Квантование и кодирование — `O(N²)`
    
- Общая сложность: `O(N²)`, где `N²` — число пикселей
    

---

## Выводы

- Сжатие с потерями уменьшает объём за счёт качества и точности.
    
- JPEG — стандарт для изображений, использует преобразование и квантование.
    
- Позволяет сильно сжимать данные при приемлемом визуальном качестве.
    
- Сложность линейна от количества пикселей, подходит для практического использования.
